#!/usr/bin/env python3
"""
å¤šçº¿ç¨‹åŒ»ç–—æ•°æ®ç”Ÿæˆå™¨ - å¤§å¹…æå‡ç”Ÿæˆé€Ÿåº¦
ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‚£è€…æ•°æ®ï¼Œé…ç½®åŠ¨æ€è°ƒæ•´å’Œè¿›åº¦ç›‘æ§
"""

import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from openai import OpenAI
from tqdm import tqdm
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Config:
    """ç»Ÿä¸€é…ç½®ç®¡ç†"""
    api_key: str = "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"
    base_url: str = "https://openrouter.ai/api/v1"
    model: str = "google/gemini-2.5-pro"
    
    # å¤šçº¿ç¨‹é…ç½®
    max_workers: int = 8          # å¹¶å‘çº¿ç¨‹æ•°
    max_retries: int = 5          # å¢åŠ é‡è¯•æ¬¡æ•°
    timeout: int = 300            # è¶…é•¿è¶…æ—¶æ—¶é—´
    base_delay: float = 0.2       # åŸºç¡€å»¶è¿Ÿ
    
    # é€Ÿç‡é™åˆ¶
    requests_per_minute: int = 200
    batch_size: int = 10          # æ‰¹å¤„ç†å¤§å°

class ThreadSafeAPIClient:
    """çº¿ç¨‹å®‰å…¨çš„APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = OpenAI(api_key=config.api_key, base_url=config.base_url)
        self.lock = Lock()
        self.request_times = []
        
    def call_api(self, messages: List[Dict], **kwargs) -> str:
        """çº¿ç¨‹å®‰å…¨çš„APIè°ƒç”¨"""
        self._rate_limit()
        
        for attempt in range(self.config.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.config.model,
                    messages=messages,
                    temperature=0.3,
                    timeout=self.config.timeout,
                    **kwargs
                )
                return response.choices[0].message.content if response.choices else ""
                
            except Exception as e:
                wait_time = self.config.base_delay * (2 ** attempt)
                logger.warning(f"Attempt {attempt + 1} failed: {str(e)[:100]}...")
                
                if attempt < self.config.max_retries - 1:
                    time.sleep(wait_time)
                else:
                    logger.error(f"All {self.config.max_retries} attempts failed")
                    return ""
    
    def _rate_limit(self):
        """é€Ÿç‡é™åˆ¶"""
        with self.lock:
            now = time.time()
            # æ¸…ç†60ç§’å‰çš„è¯·æ±‚è®°å½•
            self.request_times = [t for t in self.request_times if now - t < 60]
            
            # å¦‚æœè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾…
            if len(self.request_times) >= self.config.requests_per_minute:
                sleep_time = 60 - (now - self.request_times[0])
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            self.request_times.append(now)

class ThreadedDatabaseGenerator:
    """å¤šçº¿ç¨‹æ•°æ®åº“ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.api_client = ThreadSafeAPIClient(config)
        self.stats = {
            'total_processed': 0,
            'total_failed': 0,
            'start_time': None
        }
        self.stats_lock = Lock()
        
    def generate_tool_data(self, patient: Dict, tool: Dict) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªæ‚£è€…å’Œå·¥å…·ç”Ÿæˆæ•°æ®"""
        patient_info = f"æ‚£è€…{patient['id']}ï¼š{patient['gender']}, {patient['age']}å², è¯Šæ–­ï¼š{patient['diagnosis']}"
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ»å­¦æ•°æ®å·¥ç¨‹å¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æ‚£è€…ä¿¡æ¯ï¼Œä¸ºå·¥å…·"{tool['tool_name']}"ç”Ÿæˆç¬¦åˆå…¶output_schemaçš„çœŸå®ã€åˆç†çš„åŒ»ç–—æ•°æ®ã€‚

æ‚£è€…ä¿¡æ¯ï¼š
{patient_info}

ç—…ä¾‹æ‘˜è¦ï¼š
{patient['summary']}

å·¥å…·å®šä¹‰ï¼š
- å·¥å…·åç§°ï¼š{tool['tool_name']}
- å·¥å…·æè¿°ï¼š{tool['tool_description']}
- è¾“å‡ºæ ¼å¼ï¼š{json.dumps(tool['output_schema'], ensure_ascii=False, indent=2)}

è¦æ±‚ï¼š
1. ç”Ÿæˆçš„æ•°æ®å¿…é¡»ä¸¥æ ¼ç¬¦åˆoutput_schemaçš„æ ¼å¼
2. æ•°å€¼åº”è¯¥åœ¨åŒ»å­¦ä¸Šåˆç†ï¼ˆä¾‹å¦‚ï¼šè‚¿ç˜¤å¤§å°ã€å®éªŒå®¤æŒ‡æ ‡ç­‰ï¼‰
3. å¦‚æœæ‚£è€…æ‘˜è¦ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºå…¶è¯Šæ–­ã€å¹´é¾„ã€æ€§åˆ«ç­‰æ¨æµ‹åˆç†æ•°å€¼
4. å¯¹äºè‚ºç™Œä¸‰æœŸæ‚£è€…ï¼Œæ•°æ®åº”è¯¥ä½“ç°å…¸å‹çš„ç–¾ç—…ç‰¹å¾
5. ç›´æ¥è¿”å›JSONæ ¼å¼çš„æ•°æ®ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Š

è¯·ç”Ÿæˆæ•°æ®ï¼š
"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»å­¦æ•°æ®å·¥ç¨‹å¸ˆï¼Œæ“…é•¿åŸºäºæ‚£è€…ä¿¡æ¯ç”Ÿæˆç¬¦åˆåŒ»å­¦æ ‡å‡†çš„ç»“æ„åŒ–æ•°æ®ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        response = self.api_client.call_api(messages)
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.warning(f"JSONè§£æå¤±è´¥ - Patient {patient['id']}, Tool {tool['tool_id']}")
            return {"status": "generation_failed", "raw_response": response[:200]}
    
    def process_patient_tool_pair(self, args: tuple) -> tuple:
        """å¤„ç†å•ä¸ªæ‚£è€…-å·¥å…·å¯¹"""
        patient, tool = args
        
        try:
            result = self.generate_tool_data(patient, tool)
            
            with self.stats_lock:
                self.stats['total_processed'] += 1
                
            return (patient['id'], tool['tool_id'], result, True)
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥ - Patient {patient['id']}, Tool {tool['tool_id']}: {e}")
            
            with self.stats_lock:
                self.stats['total_failed'] += 1
                
            return (patient['id'], tool['tool_id'], {}, False)
    
    def generate_databases_parallel(self, patients_data: List[Dict], tools: List[Dict]):
        """å¹¶è¡Œç”Ÿæˆæ‰€æœ‰æ•°æ®åº“"""
        logger.info(f"ğŸš€ å¯åŠ¨å¤šçº¿ç¨‹ç”Ÿæˆ - {self.config.max_workers} çº¿ç¨‹")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        db_dir = "medical_databases"
        os.makedirs(db_dir, exist_ok=True)
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = [(patient, tool) for patient in patients_data for tool in tools]
        total_tasks = len(tasks)
        
        logger.info(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks} ({len(patients_data)} æ‚£è€… Ã— {len(tools)} å·¥å…·)")
        
        # åˆå§‹åŒ–æ•°æ®åº“ç»“æ„
        databases = {tool['tool_id']: {} for tool in tools}
        self.stats['start_time'] = time.time()
        
        # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            with tqdm(total=total_tasks, desc="ç”ŸæˆåŒ»ç–—æ•°æ®") as pbar:
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(self.process_patient_tool_pair, task): task 
                    for task in tasks
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_task):
                    patient_id, tool_id, result, success = future.result()
                    
                    if success:
                        databases[tool_id][str(patient_id)] = result
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    elapsed = time.time() - self.stats['start_time']
                    rate = self.stats['total_processed'] / elapsed if elapsed > 0 else 0
                    pbar.set_postfix({
                        'æˆåŠŸ': self.stats['total_processed'],
                        'å¤±è´¥': self.stats['total_failed'], 
                        'é€Ÿç‡': f"{rate:.1f}/s"
                    })
        
        # ä¿å­˜æ•°æ®åº“æ–‡ä»¶
        self._save_databases(databases, tools, db_dir)
        
        return databases
    
    def _save_databases(self, databases: Dict, tools: List[Dict], db_dir: str):
        """ä¿å­˜æ•°æ®åº“æ–‡ä»¶"""
        logger.info("ğŸ’¾ ä¿å­˜æ•°æ®åº“æ–‡ä»¶...")
        
        # ä¿å­˜å„å·¥å…·æ•°æ®åº“
        db_files = []
        for tool in tools:
            tool_id = tool['tool_id']
            tool_name = tool['tool_name']
            
            db_file = f"{tool_id}_{tool_name}_database.json"
            db_path = os.path.join(db_dir, db_file)
            
            with open(db_path, 'w', encoding='utf-8') as f:
                json.dump(databases[tool_id], f, ensure_ascii=False, indent=2)
            
            db_files.append(db_file)
            logger.info(f"âœ… ä¿å­˜ {tool_id} æ•°æ®åº“: {len(databases[tool_id])} æ¡è®°å½•")
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        index_data = {
            "tools": [{"tool_id": tool['tool_id'], "tool_name": tool['tool_name']} for tool in tools],
            "patients": list(databases[tools[0]['tool_id']].keys()),
            "database_files": db_files,
            "generation_stats": {
                "total_processed": self.stats['total_processed'],
                "total_failed": self.stats['total_failed'],
                "duration_seconds": time.time() - self.stats['start_time'],
                "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        with open(os.path.join(db_dir, "database_index.json"), 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

class ThreadedPatientCaseGenerator:
    """å¤šçº¿ç¨‹æ‚£è€…æ¡ˆä¾‹ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.api_client = ThreadSafeAPIClient(config)
    
    def generate_evaluation_rubrics(self, patient_data: Dict, eval_item: Dict) -> List[Dict]:
        """ç”Ÿæˆè¯„æµ‹æ ‡å‡†"""
        prompt = f"""
ä½ æ˜¯ä¸€ä½åŒ»å­¦è¯„æµ‹ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„è¯„æµ‹æ ‡å‡†ï¼ˆrubricsï¼‰ã€‚

æ‚£è€…ä¿¡æ¯ï¼š
- ID: {patient_data['id']}
- è¯Šæ–­: {patient_data['diagnosis']}
- æ²»ç–—æ ‡ç­¾: {patient_data['label']}

å‚è€ƒç­”æ¡ˆï¼š
{eval_item.get('reference_answer', patient_data['result'])}

è¦æ±‚ç”Ÿæˆ3-5ä¸ªå…·ä½“çš„è¯„æµ‹æ ‡å‡†ï¼Œæ¯ä¸ªæ ‡å‡†åŒ…å«ï¼š
1. criterion: è¯„æµ‹ç‚¹åç§°
2. description: è¯¦ç»†æè¿°
3. weight: æƒé‡(0.1-0.4ä¹‹é—´)

ç¡®ä¿æƒé‡æ€»å’Œä¸º1.0ï¼Œæ ‡å‡†åº”æ¶µç›–ï¼š
- è¯Šæ–­å‡†ç¡®æ€§
- æ²»ç–—æ–¹æ¡ˆåˆç†æ€§  
- å·¥å…·è°ƒç”¨å®Œæ•´æ€§
- ä¸´åºŠå†³ç­–é€»è¾‘

ç›´æ¥è¿”å›JSONæ ¼å¼çš„åˆ—è¡¨ï¼š
"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»å­¦è¯„æµ‹ä¸“å®¶ï¼Œæ“…é•¿åˆ¶å®šå®¢è§‚ã€å…¨é¢çš„è¯„æµ‹æ ‡å‡†ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        response = self.api_client.call_api(messages)
        
        try:
            rubrics = json.loads(response)
            # éªŒè¯æƒé‡æ€»å’Œ
            total_weight = sum(r.get('weight', 0) for r in rubrics)
            if abs(total_weight - 1.0) > 0.1:
                for r in rubrics:
                    r['weight'] = r.get('weight', 0) / total_weight if total_weight > 0 else 1.0/len(rubrics)
            return rubrics
        except json.JSONDecodeError:
            return [
                {"criterion": "è¯Šæ–­å‡†ç¡®æ€§", "description": "è¯Šæ–­æ˜¯å¦å‡†ç¡®", "weight": 0.3},
                {"criterion": "æ²»ç–—æ–¹æ¡ˆåˆç†æ€§", "description": "æ²»ç–—æ–¹æ¡ˆæ˜¯å¦åˆç†", "weight": 0.3},
                {"criterion": "å·¥å…·è°ƒç”¨å®Œæ•´æ€§", "description": "æ˜¯å¦å……åˆ†åˆ©ç”¨å·¥å…·è·å–ä¿¡æ¯", "weight": 0.2},
                {"criterion": "ä¸´åºŠå†³ç­–é€»è¾‘", "description": "å†³ç­–è¿‡ç¨‹æ˜¯å¦é€»è¾‘æ¸…æ™°", "weight": 0.2}
            ]

def load_all_data():
    """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®æ–‡ä»¶"""
    logger.info("ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")
    
    with open('data.json', 'r', encoding='utf-8') as f:
        patients_data = json.load(f)
    
    with open('eval_dataset.json', 'r', encoding='utf-8') as f:
        eval_data = json.load(f)
    
    with open('qi_med_tools.json', 'r', encoding='utf-8') as f:
        tools_data = json.load(f)
    
    initial_queries = {}
    if os.path.exists('initial_queries.json'):
        with open('initial_queries.json', 'r', encoding='utf-8') as f:
            queries_list = json.load(f)
            for query in queries_list:
                initial_queries[query['patient_id']] = query
    
    logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(patients_data)} æ‚£è€…, {len(eval_data)} è¯„æµ‹, {len(tools_data['tools'])} å·¥å…·")
    
    return patients_data, eval_data, initial_queries, tools_data['tools']

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤šçº¿ç¨‹åŒ»ç–—æ•°æ®ç”Ÿæˆå™¨å¯åŠ¨")
    print("=" * 60)
    
    config = Config()
    
    try:
        # åŠ è½½æ•°æ®
        patients_data, eval_data, initial_queries, tools = load_all_data()
        
        # ç”ŸæˆåŒ»ç–—æ•°æ®åº“
        logger.info("ğŸ—ï¸ å¼€å§‹ç”ŸæˆåŒ»ç–—æ•°æ®åº“...")
        db_generator = ThreadedDatabaseGenerator(config)
        databases = db_generator.generate_databases_parallel(patients_data, tools)
        
        elapsed = time.time() - db_generator.stats['start_time']
        logger.info(f"ğŸ‰ æ•°æ®åº“ç”Ÿæˆå®Œæˆ! ç”¨æ—¶ {elapsed:.1f}s")
        logger.info(f"ğŸ“Š æˆåŠŸ: {db_generator.stats['total_processed']}, å¤±è´¥: {db_generator.stats['total_failed']}")
        
        print("\n" + "=" * 60)
        print("âœ… å¤šçº¿ç¨‹æ•°æ®ç”Ÿæˆå®Œæˆ!")
        print(f"âš¡ é€Ÿåº¦æå‡çº¦ {config.max_workers}x")
        print("ğŸ“ æ•°æ®å·²ä¿å­˜åˆ° medical_databases/ ç›®å½•")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()