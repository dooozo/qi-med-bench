#!/usr/bin/env python3
"""
QI-Med-Bench è¯„æµ‹ç³»ç»Ÿ
åŸºäºOpenRouter APIè¿›è¡Œæ¨¡å‹è¯„æµ‹ï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨åœºæ™¯
"""

import json
import os
import sys
import time
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from openai import OpenAI
import concurrent.futures
from threading import Lock

# OpenRouteré…ç½®
API_KEY = "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"
BASE_URL = "https://openrouter.ai/api/v1"

class QIMedEvaluator:
    """QIåŒ»å­¦é¢†åŸŸè¯„æµ‹å™¨"""
    
    def __init__(self, model: str = "google/gemini-2.5-pro", max_retries: int = 3):
        self.model = model
        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
        self.max_retries = max_retries
        self.results_lock = Lock()
        
    def call_model_with_retry(self, messages: List[Dict], timeout: int = 180) -> str:
        """è°ƒç”¨æ¨¡å‹APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.1,
                    timeout=timeout
                )
                return response.choices[0].message.content if response.choices else ""
            except Exception as e:
                print(f"âš ï¸ Model call attempt {attempt + 1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    wait_time = (attempt + 1) * 3
                    print(f"ğŸ”„ Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ All {self.max_retries} attempts failed")
                    return ""

    def simulate_tool_call(self, tool_name: str, patient_id: str, 
                          tool_results_map: Dict) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨ï¼Œä»é¢„è®¾æ•°æ®ä¸­è¿”å›ç»“æœ"""
        
        # ä»å·¥å…·ç»“æœæ˜ å°„ä¸­æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®
        for tool_id, tool_data in tool_results_map.items():
            # ç®€å•çš„å·¥å…·åç§°åŒ¹é…ï¼ˆå¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„æ˜ å°„ï¼‰
            if tool_name.lower() in tool_id.lower() or any(
                keyword in tool_name.lower() 
                for keyword in ['ct', 'tumor', 'pathology', 'genetic', 'pdl1', 
                               'tnm', 'performance', 'pulmonary', 'blood', 
                               'liver', 'treatment', 'immune', 'chemo', 
                               'radiation', 'surgery']
            ):
                return tool_data
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å·¥å…·ï¼Œè¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯
        return {"error": f"Tool {tool_name} not available for patient {patient_id}"}

    def generate_diagnosis_report(self, patient_case: Dict) -> Dict[str, Any]:
        """è®©æ¨¡å‹åŸºäºåˆå§‹queryå’Œå·¥å…·è°ƒç”¨ç”Ÿæˆè¯Šç–—æŠ¥å‘Š"""
        
        patient_id = patient_case['patient_id']
        initial_query = patient_case['initial_query']
        tool_results_map = patient_case['tool_call_results_map']
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è‚ºéƒ¨è‚¿ç˜¤ç§‘ä¸“å®¶ã€‚æ‚£è€…å‘ä½ å’¨è¯¢ï¼Œä½ éœ€è¦ï¼š

1. ä»”ç»†åˆ†ææ‚£è€…çš„åˆå§‹ä¿¡æ¯
2. ä¸»åŠ¨è°ƒç”¨ç›¸å…³åŒ»ç–—å·¥å…·è·å–è¯¦ç»†æ£€æŸ¥ç»“æœ
3. åŸºäºå·¥å…·è¿”å›çš„å®¢è§‚æ•°æ®è¿›è¡ŒåŒ»å­¦æ¨ç†
4. ç»™å‡ºç»¼åˆçš„è¯Šç–—å»ºè®®

å¯ç”¨çš„åŒ»ç–—å·¥å…·åŒ…æ‹¬ï¼š
- get_chest_ct_metrics: è·å–èƒ¸éƒ¨CTæŒ‡æ ‡
- get_tumor_markers: è·å–è‚¿ç˜¤æ ‡å¿—ç‰©
- get_pathology_data: è·å–ç—…ç†æ•°æ®  
- get_genetic_mutations: è·å–åŸºå› çªå˜ä¿¡æ¯
- get_pdl1_expression: è·å–PD-L1è¡¨è¾¾
- get_tnm_staging_details: è·å–TNMåˆ†æœŸ
- get_performance_status: è·å–ä½“èƒ½çŠ¶æ€
- get_pulmonary_function: è·å–è‚ºåŠŸèƒ½
- get_blood_routine: è·å–è¡€å¸¸è§„
- get_liver_kidney_function: è·å–è‚è‚¾åŠŸèƒ½
- get_treatment_history: è·å–æ—¢å¾€æ²»ç–—å²
- get_immune_adverse_events: è·å–å…ç–«ä¸è‰¯ååº”
- get_chemo_toxicity: è·å–åŒ–ç–—æ¯’æ€§
- get_radiation_parameters: è·å–æ”¾ç–—å‚æ•°
- get_surgery_feasibility: è·å–æ‰‹æœ¯å¯è¡Œæ€§

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
1. åˆ†æåˆå§‹æŸ¥è¯¢ï¼Œè¯†åˆ«éœ€è¦è·å–çš„ä¿¡æ¯
2. ä¾æ¬¡è°ƒç”¨ç›¸å…³å·¥å…·ï¼ˆè¯·åœ¨éœ€è¦æ—¶æ˜ç¡®è¯´æ˜è°ƒç”¨å“ªä¸ªå·¥å…·ï¼‰
3. åŸºäºå·¥å…·è¿”å›çš„æ•°æ®è¿›è¡Œç»¼åˆåˆ†æ
4. ç»™å‡ºæœ€ç»ˆçš„è¯Šç–—å»ºè®®

æ³¨æ„ï¼šæ¯æ¬¡å·¥å…·è°ƒç”¨è¯·æ˜ç¡®è¯´æ˜è°ƒç”¨çš„å·¥å…·åç§°ï¼Œæˆ‘ä¼šä¸ºä½ è¿”å›å¯¹åº”çš„æ•°æ®ã€‚
"""

        # å¼€å§‹å¯¹è¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"æ‚£è€…å’¨è¯¢ï¼š{initial_query}"}
        ]
        
        tool_calls_made = []
        conversation_history = messages.copy()
        max_turns = 10  # æœ€å¤§å¯¹è¯è½®æ•°
        
        print(f"ğŸ”„ Starting diagnosis for patient {patient_id}")
        
        for turn in range(max_turns):
            print(f"  ğŸ“ Conversation turn {turn + 1}")
            
            # è°ƒç”¨æ¨¡å‹
            response = self.call_model_with_retry(conversation_history, timeout=240)
            if not response:
                break
                
            conversation_history.append({"role": "assistant", "content": response})
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨è¯·æ±‚
            tool_called = False
            for tool_name in ['get_chest_ct_metrics', 'get_tumor_markers', 'get_pathology_data',
                            'get_genetic_mutations', 'get_pdl1_expression', 'get_tnm_staging_details',
                            'get_performance_status', 'get_pulmonary_function', 'get_blood_routine',
                            'get_liver_kidney_function', 'get_treatment_history', 'get_immune_adverse_events',
                            'get_chemo_toxicity', 'get_radiation_parameters', 'get_surgery_feasibility']:
                if tool_name in response.lower() or tool_name.replace('_', ' ') in response.lower():
                    # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
                    tool_result = self.simulate_tool_call(tool_name, patient_id, tool_results_map)
                    tool_calls_made.append({
                        "tool_name": tool_name,
                        "result": tool_result,
                        "turn": turn + 1
                    })
                    
                    # å°†å·¥å…·ç»“æœåé¦ˆç»™æ¨¡å‹
                    tool_result_text = f"å·¥å…· {tool_name} è¿”å›ç»“æœï¼š\n{json.dumps(tool_result, ensure_ascii=False, indent=2)}"
                    conversation_history.append({"role": "user", "content": tool_result_text})
                    tool_called = True
                    print(f"    ğŸ”§ Called tool: {tool_name}")
                    break
            
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ä¸”å›å¤ä¸­åŒ…å«æœ€ç»ˆå»ºè®®ï¼Œç»“æŸå¯¹è¯
            if not tool_called and any(keyword in response.lower() 
                                     for keyword in ['å»ºè®®', 'æ¨è', 'æ–¹æ¡ˆ', 'æ²»ç–—', 'è¯Šæ–­', 'ç»“è®º']):
                print(f"    âœ… Final recommendation provided")
                break
                
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæç¤ºç»§ç»­
            if not tool_called:
                conversation_history.append({
                    "role": "user", 
                    "content": "è¯·ç»§ç»­åˆ†æï¼Œå¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯è¯·è°ƒç”¨ç›¸å…³å·¥å…·ï¼Œæˆ–è€…ç»™å‡ºæœ€ç»ˆçš„è¯Šç–—å»ºè®®ã€‚"
                })
        
        return {
            "patient_id": patient_id,
            "conversation_history": conversation_history,
            "tool_calls_made": tool_calls_made,
            "final_response": conversation_history[-1]["content"] if conversation_history else "",
            "turns_used": len([msg for msg in conversation_history if msg["role"] == "assistant"])
        }

    def evaluate_response(self, patient_case: Dict, model_response: Dict) -> Dict[str, Any]:
        """è¯„æµ‹æ¨¡å‹å“åº”çš„è´¨é‡"""
        
        rubrics = patient_case['evaluation_rubrics']
        reference_answer = patient_case['reference_conclusion']
        final_response = model_response['final_response']
        tool_calls = model_response['tool_calls_made']
        
        # æ„å»ºè¯„æµ‹æç¤º
        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä½åŒ»å­¦è¯„æµ‹ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹è¯„æµ‹æ ‡å‡†å¯¹AIæ¨¡å‹çš„è¯Šç–—å»ºè®®è¿›è¡Œè¯„åˆ†ã€‚

å‚è€ƒç­”æ¡ˆï¼š
{reference_answer}

AIæ¨¡å‹çš„å›ç­”ï¼š
{final_response}

AIè°ƒç”¨çš„å·¥å…·ï¼š
{json.dumps([call['tool_name'] for call in tool_calls], ensure_ascii=False)}

è¯„æµ‹æ ‡å‡†ï¼š
{json.dumps(rubrics, ensure_ascii=False, indent=2)}

è¯·ä¸ºæ¯ä¸ªè¯„æµ‹æ ‡å‡†æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼Œå¹¶è®¡ç®—åŠ æƒæ€»åˆ†ã€‚

è¿”å›æ ¼å¼ï¼š
{{
  "detailed_scores": [
    {{"criterion": "æ ‡å‡†åç§°", "score": åˆ†æ•°, "weight": æƒé‡, "comment": "è¯„ä»·è¯´æ˜"}},
    ...
  ],
  "total_score": åŠ æƒæ€»åˆ†,
  "overall_comment": "æ€»ä½“è¯„ä»·"
}}
"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»å­¦è¯„æµ‹ä¸“å®¶ï¼Œèƒ½å¤Ÿå®¢è§‚ã€å‡†ç¡®åœ°è¯„ä¼°AIæ¨¡å‹çš„åŒ»ç–—å»ºè®®è´¨é‡ã€‚"},
            {"role": "user", "content": evaluation_prompt}
        ]
        
        evaluation_response = self.call_model_with_retry(messages, timeout=120)
        
        try:
            evaluation_result = json.loads(evaluation_response)
            return evaluation_result
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„åˆ†
            return {
                "detailed_scores": [{"criterion": r["criterion"], "score": 5, "weight": r.get("weight", 0.25), "comment": "è¯„æµ‹å¤±è´¥"} for r in rubrics],
                "total_score": 5.0,
                "overall_comment": "è¯„æµ‹ç³»ç»Ÿè§£æå¤±è´¥"
            }

    def evaluate_single_case(self, patient_case: Dict) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªæ‚£è€…æ¡ˆä¾‹"""
        
        patient_id = patient_case['patient_id']
        print(f"\nğŸ“‹ Evaluating patient {patient_id}")
        
        start_time = time.time()
        
        try:
            # 1. ç”Ÿæˆè¯Šç–—æŠ¥å‘Š
            model_response = self.generate_diagnosis_report(patient_case)
            
            # 2. è¯„æµ‹å“åº”è´¨é‡
            evaluation_result = self.evaluate_response(patient_case, model_response)
            
            # 3. æ•´åˆç»“æœ
            end_time = time.time()
            
            result = {
                "patient_id": patient_id,
                "model_response": model_response,
                "evaluation": evaluation_result,
                "metadata": {
                    "evaluation_time": end_time - start_time,
                    "model_used": self.model,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            print(f"âœ… Patient {patient_id} completed - Score: {evaluation_result.get('total_score', 0):.2f}")
            return result
            
        except Exception as e:
            print(f"âŒ Error evaluating patient {patient_id}: {str(e)}")
            return {
                "patient_id": patient_id,
                "error": str(e),
                "metadata": {
                    "evaluation_time": time.time() - start_time,
                    "model_used": self.model,
                    "timestamp": datetime.now().isoformat()
                }
            }

    def run_evaluation(self, cases_file: str, output_file: str, 
                      max_workers: int = 3, start_idx: int = 0, end_idx: int = -1):
        """è¿è¡Œå®Œæ•´è¯„æµ‹"""
        
        print("ğŸš€ Starting QI-Med-Bench Evaluation")
        print("=" * 50)
        print(f"ğŸ“Š Model: {self.model}")
        print(f"ğŸ‘¥ Max workers: {max_workers}")
        
        # åŠ è½½æ‚£è€…æ¡ˆä¾‹
        with open(cases_file, 'r', encoding='utf-8') as f:
            all_cases = json.load(f)
        
        # é€‰æ‹©è¯„æµ‹èŒƒå›´
        if end_idx == -1:
            end_idx = len(all_cases)
        cases_to_evaluate = all_cases[start_idx:end_idx]
        
        print(f"ğŸ“‹ Evaluating cases {start_idx} to {end_idx-1} ({len(cases_to_evaluate)} total)")
        
        # å¹¶è¡Œè¯„æµ‹
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_case = {
                executor.submit(self.evaluate_single_case, case): case 
                for case in cases_to_evaluate
            }
            
            for future in concurrent.futures.as_completed(future_to_case):
                result = future.result()
                with self.results_lock:
                    results.append(result)
                    
                    # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                    if len(results) % 5 == 0:
                        temp_file = f"{output_file}.temp"
                        with open(temp_file, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if 'evaluation' in r]
        scores = [r['evaluation']['total_score'] for r in successful_results]
        
        summary = {
            "total_cases": len(cases_to_evaluate),
            "successful_evaluations": len(successful_results),
            "failed_evaluations": len(results) - len(successful_results),
            "average_score": sum(scores) / len(scores) if scores else 0,
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "model_used": self.model,
            "evaluation_date": datetime.now().isoformat()
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_result = {
            "summary": summary,
            "results": results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Evaluation Completed!")
        print(f"ğŸ“Š Results: {len(successful_results)}/{len(cases_to_evaluate)} successful")
        print(f"ğŸ“ˆ Average Score: {summary['average_score']:.2f}")
        print(f"ğŸ“ Results saved to: {output_file}")
        
        return final_result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="QI-Med-Bench Evaluator")
    parser.add_argument("--model", type=str, default="google/gemini-2.5-pro", 
                       help="Model to evaluate")
    parser.add_argument("--cases-file", type=str, default="all_patient_cases.json",
                       help="Patient cases file")
    parser.add_argument("--output-file", type=str, default="evaluation_results.json",
                       help="Output file for results")
    parser.add_argument("--max-workers", type=int, default=2,
                       help="Maximum number of parallel workers")
    parser.add_argument("--start-idx", type=int, default=0,
                       help="Start index for evaluation")
    parser.add_argument("--end-idx", type=int, default=-1,
                       help="End index for evaluation (-1 for all)")
    
    args = parser.parse_args()
    
    try:
        evaluator = QIMedEvaluator(model=args.model)
        evaluator.run_evaluation(
            cases_file=args.cases_file,
            output_file=args.output_file,
            max_workers=args.max_workers,
            start_idx=args.start_idx,
            end_idx=args.end_idx
        )
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Evaluation interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error during evaluation: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()