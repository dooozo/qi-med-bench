#!/usr/bin/env python3
"""
QI-Med-Bench å®Œæ•´ç³»ç»Ÿæµ‹è¯•è„šæœ¬
æµ‹è¯•æ•´ä¸ªå·¥ä½œæµç¨‹ï¼šæ•°æ®åŠ è½½ -> å·¥å…·è°ƒç”¨ -> æ¨¡å‹è¯„æµ‹
"""

import json
import os
import sys
import time
from typing import Dict, List, Any

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ§ª Testing Data Loading...")
    
    required_files = [
        "data.json",
        "eval_dataset.json", 
        "qi_med_tools.json"
    ]
    
    for file_name in required_files:
        if os.path.exists(file_name):
            with open(file_name, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"  âœ… {file_name}: {len(data)} items")
        else:
            print(f"  âŒ {file_name}: Missing")
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    generated_files = {
        "initial_queries.json": "Initial queries",
        "all_patient_cases.json": "Patient cases",
        "medical_databases/database_index.json": "Database index"
    }
    
    for file_path, description in generated_files.items():
        if os.path.exists(file_path):
            print(f"  âœ… {description}: Generated")
        else:
            print(f"  â³ {description}: Not yet generated")

def test_medical_environment():
    """æµ‹è¯•åŒ»ç–—ç¯å¢ƒ"""
    print("\nğŸ§ª Testing Medical Environment...")
    
    try:
        from tau_bench.envs.medical import QIMedicalDomainEnv
        from tau_bench.envs.user import UserStrategy
        
        # åˆ›å»ºç¯å¢ƒå®ä¾‹
        env = QIMedicalDomainEnv(
            user_strategy=UserStrategy.LLM,
            user_model="gpt-4o",
            task_split="test"
        )
        
        print(f"  âœ… Environment created successfully")
        print(f"  ğŸ“Š Tools available: {len(env.tools_info)}")
        print(f"  ğŸ“‹ Tasks loaded: {len(env.tasks)}")
        
        # æµ‹è¯•å·¥å…·ä¿¡æ¯
        for i, tool in enumerate(env.tools_info[:3]):  # æ˜¾ç¤ºå‰3ä¸ªå·¥å…·
            print(f"    ğŸ”§ Tool {i+1}: {tool['name']}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Environment test failed: {e}")
        return False

def test_tool_functions():
    """æµ‹è¯•å·¥å…·å‡½æ•°"""
    print("\nğŸ§ª Testing Tool Functions...")
    
    try:
        from tau_bench.envs.medical.tools.medical_tools import (
            get_chest_ct_metrics, get_tumor_markers, db_manager
        )
        
        # æµ‹è¯•æ•°æ®åº“ç®¡ç†å™¨
        print(f"  ğŸ“Š Database manager loaded {len(db_manager.databases)} databases")
        
        # æµ‹è¯•å·¥å…·è°ƒç”¨ï¼ˆä½¿ç”¨æ‚£è€…1ä½œä¸ºç¤ºä¾‹ï¼‰
        test_patient_id = "1"
        
        # æµ‹è¯•CTå·¥å…·
        ct_result = get_chest_ct_metrics(test_patient_id)
        if ct_result:
            print(f"  âœ… CT metrics tool working")
        else:
            print(f"  âš ï¸ CT metrics returned empty")
        
        # æµ‹è¯•è‚¿ç˜¤æ ‡å¿—ç‰©å·¥å…·
        marker_result = get_tumor_markers(test_patient_id)
        if marker_result:
            print(f"  âœ… Tumor markers tool working")
        else:
            print(f"  âš ï¸ Tumor markers returned empty")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Tool functions test failed: {e}")
        return False

def test_tau_bench_integration():
    """æµ‹è¯•ä¸tau_benchæ¡†æ¶çš„é›†æˆ"""
    print("\nğŸ§ª Testing Tau-Bench Integration...")
    
    try:
        from tau_bench.types import RunConfig
        from tau_bench.envs import get_env
        
        # åˆ›å»ºé…ç½®
        config = RunConfig(
            model_provider="openai",
            user_model_provider="openai", 
            model="gpt-4o",
            env="medical",
            agent_strategy="tool-calling",
            start_index=0,
            end_index=2  # åªæµ‹è¯•å‰2ä¸ªä»»åŠ¡
        )
        
        print(f"  âœ… Config created: {config.env} environment")
        
        # æµ‹è¯•ç¯å¢ƒè·å–
        env = get_env(
            env_name="medical",
            user_strategy="llm",
            user_model="gpt-4o",
            task_split="test"
        )
        
        print(f"  âœ… Environment retrieved successfully")
        print(f"  ğŸ“Š Available tools: {len(env.tools_info)}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Integration test failed: {e}")
        return False

def test_evaluation_system():
    """æµ‹è¯•è¯„æµ‹ç³»ç»Ÿ"""
    print("\nğŸ§ª Testing Evaluation System...")
    
    try:
        from qi_med_evaluator import QIMedEvaluator
        
        # åˆ›å»ºè¯„æµ‹å™¨
        evaluator = QIMedEvaluator(model="google/gemini-2.5-pro")
        print(f"  âœ… Evaluator created with model: {evaluator.model}")
        
        # æµ‹è¯•å·¥å…·è°ƒç”¨æ¨¡æ‹Ÿ
        test_tool_map = {
            "LC001": {"tumor_size": {"max_diameter_mm": 35}},
            "LC002": {"CEA_ng_ml": 5.2}
        }
        
        result = evaluator.simulate_tool_call("get_chest_ct_metrics", "1", test_tool_map)
        if result:
            print(f"  âœ… Tool call simulation working")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Evaluation system test failed: {e}")
        return False

def create_demo_data():
    """åˆ›å»ºæ¼”ç¤ºæ•°æ®ï¼ˆå¦‚æœç”Ÿæˆçš„æ•°æ®è¿˜æœªå®Œæˆï¼‰"""
    print("\nğŸ”§ Creating Demo Data...")
    
    # åˆ›å»ºæœ€å°åŒ–çš„æ¼”ç¤ºæ•°æ®åº“
    demo_db_dir = "demo_medical_databases"
    if not os.path.exists(demo_db_dir):
        os.makedirs(demo_db_dir)
    
    # ç¤ºä¾‹å·¥å…·æ•°æ®
    demo_tools = ["LC001", "LC002", "LC003"]
    demo_patients = ["1", "2", "3"]
    
    for tool_id in demo_tools:
        tool_db = {}
        for patient_id in demo_patients:
            if tool_id == "LC001":  # CTæŒ‡æ ‡
                tool_db[patient_id] = {
                    "tumor_size": {"max_diameter_mm": 30 + int(patient_id) * 5, "volume_cm3": 15.5},
                    "pleural_invasion": True,
                    "lymph_nodes": [{"station": "4R", "size_mm": 12, "suv_value": 3.2}]
                }
            elif tool_id == "LC002":  # è‚¿ç˜¤æ ‡å¿—ç‰©
                tool_db[patient_id] = {
                    "CEA_ng_ml": 4.5 + int(patient_id) * 0.8,
                    "NSE_ng_ml": 15.2,
                    "CYFRA21_1_ng_ml": 3.1
                }
            elif tool_id == "LC003":  # ç—…ç†æ•°æ®
                tool_db[patient_id] = {
                    "histology_type": "è…ºç™Œ",
                    "differentiation_grade": "ä¸­åˆ†åŒ–",
                    "ki67_percentage": 60 + int(patient_id) * 5
                }
        
        # ä¿å­˜å·¥å…·æ•°æ®åº“
        db_file = os.path.join(demo_db_dir, f"{tool_id}_demo_database.json")
        with open(db_file, 'w', encoding='utf-8') as f:
            json.dump(tool_db, f, ensure_ascii=False, indent=2)
    
    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    index_data = {
        "tools": [{"tool_id": tid, "tool_name": f"demo_tool_{tid}"} for tid in demo_tools],
        "patients": demo_patients,
        "database_files": [f"{tid}_demo_database.json" for tid in demo_tools]
    }
    
    with open(os.path.join(demo_db_dir, "database_index.json"), 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… Demo database created in {demo_db_dir}")
    
    # åˆ›å»ºæ¼”ç¤ºæ‚£è€…æ¡ˆä¾‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists("all_patient_cases.json"):
        demo_cases = []
        for i in range(3):
            case = {
                "patient_id": str(i + 1),
                "initial_query": f"æ‚£è€…{i+1}ï¼Œ65å²å¥³æ€§ï¼Œå› å’³å—½å’³ç—°1æœˆä½™å°±è¯Šï¼Œè¯·é—®è¯Šç–—æ–¹æ¡ˆï¼Ÿ",
                "tool_call_results_map": {
                    "LC001": {"tumor_size": {"max_diameter_mm": 30 + i * 5}},
                    "LC002": {"CEA_ng_ml": 4.5 + i * 0.8},
                    "LC003": {"histology_type": "è…ºç™Œ"}
                },
                "reference_conclusion": "å»ºè®®å®Œå–„æ£€æŸ¥ååˆ¶å®šç»¼åˆæ²»ç–—æ–¹æ¡ˆ",
                "evaluation_rubrics": [
                    {"criterion": "è¯Šæ–­å‡†ç¡®æ€§", "description": "è¯Šæ–­æ˜¯å¦å‡†ç¡®", "weight": 0.4},
                    {"criterion": "æ²»ç–—åˆç†æ€§", "description": "æ²»ç–—æ–¹æ¡ˆæ˜¯å¦åˆç†", "weight": 0.6}
                ],
                "metadata": {"age": 65, "gender": "å¥³", "diagnosis": "è‚ºç™Œ"}
            }
            demo_cases.append(case)
        
        with open("demo_patient_cases.json", 'w', encoding='utf-8') as f:
            json.dump(demo_cases, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… Demo patient cases created")

def run_mini_evaluation():
    """è¿è¡Œå°è§„æ¨¡è¯„æµ‹æ¼”ç¤º"""
    print("\nğŸš€ Running Mini Evaluation Demo...")
    
    # ç¡®ä¿æœ‰æ¼”ç¤ºæ•°æ®
    if not os.path.exists("demo_patient_cases.json"):
        create_demo_data()
    
    try:
        # ä¿®æ”¹æ•°æ®åº“è·¯å¾„ä¸ºæ¼”ç¤ºæ•°æ®åº“
        import tau_bench.envs.medical.tools.medical_tools as mt
        original_db_dir = mt.MedicalDatabaseManager.__init__
        
        def demo_init(self):
            self.databases = {}
            self.database_dir = "demo_medical_databases"
            self._load_databases()
        
        mt.MedicalDatabaseManager.__init__ = demo_init
        
        # é‡æ–°åŠ è½½æ•°æ®åº“ç®¡ç†å™¨
        mt.db_manager = mt.MedicalDatabaseManager()
        
        from qi_med_evaluator import QIMedEvaluator
        
        evaluator = QIMedEvaluator(model="google/gemini-2.5-pro")
        
        # åŠ è½½æ¼”ç¤ºæ¡ˆä¾‹
        with open("demo_patient_cases.json", 'r', encoding='utf-8') as f:
            demo_cases = json.load(f)
        
        print(f"  ğŸ“‹ Loaded {len(demo_cases)} demo cases")
        
        # è¯„æµ‹ç¬¬ä¸€ä¸ªæ¡ˆä¾‹
        print(f"\n  ğŸ§ª Testing case 1...")
        result = evaluator.evaluate_single_case(demo_cases[0])
        
        if 'evaluation' in result:
            score = result['evaluation'].get('total_score', 0)
            print(f"  âœ… Demo evaluation completed - Score: {score:.2f}")
            
            # ä¿å­˜ç»“æœ
            with open("demo_evaluation_result.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            return True
        else:
            print(f"  âŒ Demo evaluation failed")
            return False
            
    except Exception as e:
        print(f"  âŒ Mini evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ QI-Med-Bench System Test")
    print("=" * 50)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("Data Loading", test_data_loading),
        ("Medical Environment", test_medical_environment), 
        ("Tool Functions", test_tool_functions),
        ("Tau-Bench Integration", test_tau_bench_integration),
        ("Evaluation System", test_evaluation_system)
    ]
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"  âŒ {test_name} crashed: {e}")
            test_results.append((test_name, False))
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    create_demo_data()
    
    # è¿è¡Œæ¼”ç¤ºè¯„æµ‹
    demo_result = run_mini_evaluation()
    test_results.append(("Demo Evaluation", demo_result))
    
    # æ˜¾ç¤ºç»“æœæ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š Test Results Summary")
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! QI-Med-Bench is ready!")
    else:
        print("âš ï¸ Some tests failed. Check the output above for details.")


if __name__ == "__main__":
    main()